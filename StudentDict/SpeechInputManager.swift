import Foundation
import Speech
import Combine
import AVFoundation

// MARK: - [Fix] Robust Speech Manager
// ä¿®å¾©é‡é»ï¼šåŠ å¼·è³‡æºé‡‹æ”¾é‚è¼¯ï¼Œé˜²æ­¢ç¬¬äºŒæ¬¡éŒ„éŸ³æ™‚ Audio Engine å¡æ­»
class SpeechInputManager: NSObject, ObservableObject, SFSpeechRecognizerDelegate {
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // ç™¼å¸ƒçµ¦ UI çš„ç‹€æ…‹
    @Published var transcribedText: String = ""
    @Published var isRecording: Bool = false
    @Published var errorMessage: String?
    
    // ç”¨ä¾†åµæ¸¬åœé “çš„è¨ˆæ™‚å™¨
    private var silenceTimer: Timer?
    
    override init() {
        super.init()
        speechRecognizer?.delegate = self
    }
    
    // é–‹å§‹/åœæ­¢éŒ„éŸ³çš„é–‹é—œ
    func toggleRecording() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
    
    private func startRecording() {
        // 0. å¼·åˆ¶æ¸…ç†èˆŠç‹€æ…‹ (é˜²ç¦¦æ€§ç¨‹å¼è¨­è¨ˆ)
        cleanupSpeechSession()
        
        // 1. æª¢æŸ¥æ¬Šé™
        SFSpeechRecognizer.requestAuthorization { status in
            guard status == .authorized else {
                DispatchQueue.main.async { self.errorMessage = "è«‹è‡³è¨­å®šé–‹å•ŸèªéŸ³æ¬Šé™" }
                return
            }
        }
        
        // 2. è¨­å®š Audio Session
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // è¨­å®šç‚ºéŒ„éŸ³æ¨¡å¼ï¼Œä¸¦ç¸®å°å…¶ä»–è²éŸ³ (duckOthers)
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("Audio Session Error: \(error)")
            return
        }
        
        // 3. å»ºç«‹è¾¨è­˜è«‹æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else { return }
        // ä¸éœ€è¦ç­‰åˆ°è¬›å®Œä¸€å¥æ‰å›å‚³ï¼Œè¦å³æ™‚å›å‚³
        recognitionRequest.shouldReportPartialResults = true
        
        // 4. è¨­å®šè¼¸å…¥æº (éº¥å…‹é¢¨)
        let inputNode = audioEngine.inputNode
        
        // 5. é–‹å§‹è¾¨è­˜ä»»å‹™
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            var isFinal = false
            
            if let result = result {
                // æ›´æ–°æ–‡å­—åˆ° UI
                DispatchQueue.main.async {
                    self.transcribedText = result.bestTranscription.formattedString
                }
                isFinal = result.isFinal
                
                // ğŸ”¥ æ”¶åˆ°æ–°æ–‡å­—å¾Œï¼Œé‡ç½®åœé “è¨ˆæ™‚å™¨
                self.resetSilenceTimer()
            }
            
            if error != nil || isFinal {
                // ç™¼ç”ŸéŒ¯èª¤æˆ–å·²ç¶“çµæŸæ™‚ï¼ŒåŸ·è¡Œæ¸…ç†
                self.stopRecording()
            }
        }
        
        // 6. å®‰è£ Tap (ç›£è½éº¥å…‹é¢¨æ•¸æ“š)
        // âš ï¸ é—œéµä¿®æ­£ï¼šå…ˆç§»é™¤å¯èƒ½æ®˜ç•™çš„ Tapï¼Œå†å®‰è£æ–°çš„
        inputNode.removeTap(onBus: 0)
        
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        // 7. å•Ÿå‹•å¼•æ“
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
            DispatchQueue.main.async {
                self.transcribedText = ""
                self.isRecording = true
            }
        } catch {
            print("Audio Engine Start Error: \(error)")
        }
    }
    
    func stopRecording() {
        // åœæ­¢éŒ„éŸ³æ™‚ï¼ŒåŸ·è¡Œå®Œæ•´æ¸…ç†
        cleanupSpeechSession()
        
        DispatchQueue.main.async {
            self.isRecording = false
        }
    }
    
    // MARK: - Helper: æ·±åº¦æ¸…ç†è³‡æº
    private func cleanupSpeechSession() {
        // 1. åœæ­¢ Audio Engine
        if audioEngine.isRunning {
            audioEngine.stop()
            // âš ï¸ é—œéµï¼šä¸€å®šè¦ç§»é™¤ Tapï¼Œå¦å‰‡ä¸‹æ¬¡ start æœƒå´©æ½°
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // 2. çµæŸè«‹æ±‚
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // 3. å–æ¶ˆä»»å‹™
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // 4. åœæ­¢è¨ˆæ™‚å™¨
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        // 5. é‡‹æ”¾ Audio Session (è®“å–‡å­å¯ä»¥æ¢å¾©æ’­æ”¾è²éŸ³)
        // æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ try? å¿½ç•¥éŒ¯èª¤ï¼Œé¿å…å½±éŸ¿æµç¨‹
        try? AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
    }
    
    // é‡ç½®åœé “è¨ˆæ™‚å™¨
    private func resetSilenceTimer() {
        silenceTimer?.invalidate()
        // è¨­å®š 1.5 ç§’å¾Œè‡ªå‹•åœæ­¢
        silenceTimer = Timer.scheduledTimer(withTimeInterval: 1.5, repeats: false) { [weak self] _ in
            DispatchQueue.main.async {
                // æ™‚é–“åˆ°ï¼Œè‡ªå‹•åœæ­¢
                self?.stopRecording()
            }
        }
    }
}
